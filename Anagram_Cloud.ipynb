{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a6c6fa",
   "metadata": {},
   "source": [
    "## Anagram Word Cloud Generation for Qualitative Code Themes\n",
    "\n",
    "The following script generates anagram word clouds, frequency word clouds, and comparative Venn diagrams from qualitative code text data. It begins by importing all of the packages that are necessary for creating these data visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import NLPmod\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import heapq\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.inspection import permutation_importance \n",
    "from sklearn import metrics\n",
    "import collections\n",
    "import six\n",
    "import sys\n",
    "import sklearn.neighbors._base\n",
    "from scipy.stats import rankdata\n",
    "try:\n",
    "    from sklearn.utils import safe_indexing\n",
    "except ImportError:\n",
    "    from sklearn.utils import _safe_indexing\n",
    "    sys.modules['sklearn.utils.safe_indexing'] = sklearn.utils._safe_indexing\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c73969",
   "metadata": {},
   "source": [
    "### Loading Data & Tokenization\n",
    "\n",
    "This preliminary block starts by loading in the labeled text as a pandas dataframe; it is also in this block where you choose the column label for your anagram cloud. As a reminder, here are the options available for each of the test files:\n",
    "- `pos_exp.csv`: achievement, affection, bonding, enjoy_the_moment, exercise, leisure, nature.\n",
    "- `title_topic.csv`: Computer Science, Mathematics, Physics, Statistics.\n",
    "\n",
    "With selection complete, the block then performs tokenization of the text data. The text is cleaned using a general process, then broken up into a lemmatized form which is filters out unnecessary stop words. At present, n-grams are being used with unigrams, bigrams, and trigrams that occur five times or more across all responses being labeled as 'common'. The common tokens are transformed into our vector space so that each comment can be read as a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a130c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'pos_exp.csv' # select either pos_exp or title_topic (or your own data set!)\n",
    "theme = 'achievement' # name of column that is used to make the anagram cloud\n",
    "\n",
    "minOcc = 3 # what is the most amount of times that a token can appear while still being exluded from consideration?\n",
    "ngram = False # Should we include bigrams and trigrams on top of our unigrams\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # intialize the lemmatizer as a variable.\n",
    "stopset = set(stopwords.words('english')) # set up stopwords set.\n",
    "\n",
    "lab_text_df = pd.read_csv(file)\n",
    "lab_text_df[\"token\"] = lab_text_df.text.apply(NLPmod.tokenizerPOS, args=(lemmatizer,stopset))\n",
    "if(ngram) :\n",
    "        lab_text_df[\"token\"] = lab_text_df.token.apply(NLPmod.ngramsappend, args=([2,3],False))\n",
    "fbank = NLPmod.wordfreq(lab_text_df.token) # generate bank of words with associated frequencies\n",
    "comBank, rarBank = NLPmod.repbank(fbank, minOcc) # generate two words sets based on which words are recurring\n",
    "wordHeap = [token for comment in lab_text_df.token for token in comment] # generate ordered multiset of words\n",
    "stackHeap = collections.Counter(wordHeap) # condense down into a collection\n",
    "vocabComm = {term: ind for ind, term in enumerate(comBank)}\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer= lambda val: val,\n",
    "    preprocessor= lambda val: val,\n",
    "    token_pattern=None,\n",
    "    vocabulary = vocabComm\n",
    ")\n",
    "transformed = tfidf.fit_transform(lab_text_df.token)\n",
    "lab_text_df['tfidfVec'] = [row.tolist()[0] for row in transformed.todense()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9b98c",
   "metadata": {},
   "source": [
    "### Parameter Testing\n",
    "\n",
    "With the TF-IDF vectors, we will do a grid-search in order to find the optimal parameters. If you already know what parameters you want to use, but wish to run this block anyway as part of running all blocks: swap out the `grid` dictionary with the `skip_grid` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424073f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = { # grid of paremeter choices\n",
    "    'learning_rate': np.arange(0.05,1.0,0.2),\n",
    "    'n_estimators': np.arange(40,400,80),\n",
    "    'subsample' : [0.25,0.5,0.75,1.0],\n",
    "    'max_depth' : [3,4,5]\n",
    "}\n",
    "skip_grid = {'learning_rate' : [0.5]}\n",
    "\n",
    "\n",
    "Xdata = np.array(list(lab_text_df.tfidfVec))\n",
    "Xtarg = np.array(list(lab_text_df[theme]))\n",
    "clf = GradientBoostingClassifier() \n",
    "clf_grid = GridSearchCV(clf, grid, verbose = 1) # MAKE ALTERATION HERE! \n",
    "clf_grid.fit(Xdata,Xtarg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c148d5",
   "metadata": {},
   "source": [
    "### Optimal Parameters for the Model\n",
    "\n",
    "After doing the cross-validated grid search, we record the optimal parameters so that they may be called upon at a later time without having to run another grid search for hyper-parameters. Use this block to store the optimal parameters that were found from the grid search of the TF-IDF vectorized data points within the Gradient Boosting Machine model. Alter these as necessary based on whatever results you may obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52367b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Optimal parameters for label '{theme}'\")\n",
    "print(f\"The best accuracy score across ALL searched params: {clf_grid.best_score_}\")\n",
    "print(f\"The best parameters across ALL searched params: {clf_grid.best_params_}\")\n",
    "\n",
    "LR = 0.05\n",
    "nEst = 40\n",
    "subSamp = 0.50\n",
    "depth = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95af4ad",
   "metadata": {},
   "source": [
    "### Theme Investigation with Influential Words\n",
    "\n",
    "It is time to investigate again how permutation of a column may affect the accuracy of our prediction results. To this end, we create a number of models that all have the same parameter settings. For each model, we investigate how juggling the entry of one dimension for all entries affects the cross-validated prediction accuracy of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = range(50) # How many trials shall we do?\n",
    "permImp = [] # list that holds the permutation importance score results.\n",
    "\n",
    "print(f\"Completing Trials for theme '{theme}': (\", end = \"\") \n",
    "for trial in trials: # make a model for each trial\n",
    "    print('|',end=\"\")\n",
    "    scra_clf = GradientBoostingClassifier( # set parameters for model\n",
    "        learning_rate = LR, \n",
    "        n_estimators = nEst,\n",
    "        subsample = subSamp,\n",
    "        max_depth = depth\n",
    "    )\n",
    "    scra_clf.fit(Xdata,Xtarg)\n",
    "    permImp.append(permutation_importance( # create dictionary entry for permutation importance score\n",
    "        scra_clf,\n",
    "        Xdata,\n",
    "        Xtarg,\n",
    "        n_repeats=5\n",
    "    ))\n",
    "print(')')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af6c33",
   "metadata": {},
   "source": [
    "### Permutation Importance and Frequency DataFrames\n",
    "\n",
    "This block creates DataFrames for each theme. We look at the averaged permutation importance scores across each trial. The permuation importance scores are then ranked so that a more discretized sorted version of the importance scores can be visualized. Frequency data is stored adjacent to these importance scores and also ranked using a similar system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d671bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dict = {f'perm_imp_{i}' : trial['importances_mean'] for i, trial in enumerate(permImp)} # create scratch frame for permutation importance scores of each theme\n",
    "perm_freq_df = pd.DataFrame(load_dict) # create dataframe dictionary entry for scratch frame\n",
    "perm_freq_df['perm_imp_tot'] = perm_freq_df.mean(axis = 1) # compute averaged importance score over each trial\n",
    "perm_freq_df['perm_imp_rank'] = rankdata(perm_freq_df['perm_imp_tot'], method = 'min') - 1 # create ranked version of importance scores\n",
    "\n",
    "perm_freq_df['freq'] = [stackHeap[word] for word in list(tfidf.get_feature_names_out())] # calculate frequency data for all captured features\n",
    "perm_freq_df['freq_rank'] = rankdata(perm_freq_df['freq'], method = 'min') - 1 # calculate similar frequency ranks from frequency data\n",
    "\n",
    "perm_freq_df['feature'] = comBank # Create column that says the actual feature token labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e73cbf",
   "metadata": {},
   "source": [
    "### Filtering for Theme-Adhering Comments\n",
    "\n",
    "We want to now look only at comments where a theme is present. We do this in order to analyze what commonalities these comments may have with one another. Looking at these comments, we break them up into a multiset list of all the tokens that they are made up of. These multisets are then heaped into sorted piles of frequency data using the `collections.Counter` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbea83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_filt = lab_text_df.groupby(theme).get_group(1)\n",
    "heap_filt = [token_item for token_list in lab_filt.token for token_item in token_list] # generate ordered multiset of subset\n",
    "count_filt = collections.Counter(heap_filt) # condense down into a collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7832957",
   "metadata": {},
   "source": [
    "### Theme-Specific Frequency Table Generation\n",
    "\n",
    "Looking at just the comments that adhere to a theme, we can use the counter collection dictionaries to create two columns of a DataFrame. These tables will examine only the non-unique words that appear across all comments which adhere to a given theme. We track those words, their frequency, and the frequency ranks.\n",
    "\n",
    "We then merge the sub-frequency table with the general frequency table. We do this in order to create frequency ratios: the frequency of a feature within the subset of comments against the general frequency of a feature across all comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraWords = [k for k,v in count_filt.items() if v > minOcc] # words that appear non-uniquely\n",
    "scraFreq = [v for k,v in count_filt.items() if v > minOcc] # frequency of those words\n",
    "sub_freq_df = pd.DataFrame({ # make a sub-frame\n",
    "    'feature' : scraWords, # Create column of these words\n",
    "    'sub_freq' : scraFreq # Create column for matching frequency of those words\n",
    "})\n",
    "sub_freq_df['sub_freq_rank'] = rankdata(sub_freq_df['sub_freq'], method = 'min') - 1\n",
    "\n",
    "df = perm_freq_df.merge(sub_freq_df, how = 'left', on = 'feature').copy() # Merge tables under feature\n",
    "df.fillna(0, inplace = True) # fill in all Nan values with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b3856",
   "metadata": {},
   "source": [
    "### Prepping the Plot Lists\n",
    "\n",
    "The next several blocks show the plots that compare columns of data relating to permutation importance and frequency.\n",
    "- The first of the seven plots compares the permutation importance of each feature against the frequency data of that feature.\n",
    "- The second of the seven plots does the same thing except that it uses the ranked frequency and permutation importance rather than the raw values.\n",
    "- The third of the seven plots depicts similar information except that the frequency data only considers instances of feature appearance when it happens within text that is labeled as belonging to the theme. Doing this helps hone in on the comparisons between permutation importance and frequency.\n",
    "- Plot four is to plot two in that it shows ranked permutation importance score and ranked word sub-frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620b14e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comps = [\n",
    "    ('perm_imp_tot', 'freq', f'Permutation Importance vs. Frequency for Theme {theme}'),\n",
    "    ('perm_imp_rank', 'freq_rank', f'Ranked Permutation Importance vs. Ranked Frequency for Theme {theme}'),\n",
    "    ('perm_imp_tot', 'sub_freq', f'Permutation Importance vs. Filtered Frequency for Theme {theme}'),\n",
    "    ('perm_imp_rank', 'sub_freq_rank', f'Ranked Permutation Importance vs. Ranked Filtered Frequency for Theme {theme}'),\n",
    "    ('freq', 'sub_freq', 'General Frequency vs. Theme Subset Frequency')\n",
    "]\n",
    "\n",
    "for x, y, title_lab in comps:\n",
    "    df.plot.scatter(\n",
    "        x,\n",
    "        y,\n",
    "        figsize=(8,8), \n",
    "        title = title_lab\n",
    "    )\n",
    "    corr = df[x].corr(df[y])\n",
    "    print(f'Raw Correlation Between {x} and {y} is {corr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77156d",
   "metadata": {},
   "source": [
    "### Venn Diagram Design\n",
    "\n",
    "We now look at Venn Diagrams made between the words that are frequent for a theme and deemed to have permutation importance for a theme. We create an intersection slice between these two sets and then take set differences between the intersection and the component sets. We do this for each of the themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac8ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn2 \n",
    "mostAmo = 100\n",
    "\n",
    "most_perm_imp = df.sort_values(by = 'perm_imp_tot', ascending = False, ignore_index = True)[:mostAmo].copy()\n",
    "most_sub_freq = df.sort_values(by = 'sub_freq', ascending = False, ignore_index = True)[:mostAmo].copy()\n",
    "venn2(\n",
    "    [\n",
    "        set(most_perm_imp['feature']),\n",
    "        set(most_sub_freq['feature'])\n",
    "    ], \n",
    "    (\n",
    "        f'Tokens of High Permutation Importance', \n",
    "        f'Tokens with High Sub-Frequency of Occurence'\n",
    "    )\n",
    ")\n",
    "for ind, perm_w, freq_w in zip(range(mostAmo), list(most_perm_imp['feature']), list(most_sub_freq['feature'])):\n",
    "    print(f'{ind+1} | perm_imp: {perm_w}, sub_freq: {freq_w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a62a81",
   "metadata": {},
   "source": [
    "### Comparison of Anagram and Word Clouds\n",
    "\n",
    "Having looked at these other visual comparisons, it is now time to make the anagram and word clouds from the labeled data. We make each cloud by creating dictionaries from pairs of columns that exist in our filtered permutation importance and sub-frequency dataframes. The first block shows the word cloud created from the ladder question data and the second block shows the anagram cloud that is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b419ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_feed = {row['feature'] : row['sub_freq'] for i, row in most_sub_freq.iterrows()}\n",
    "word_cloud = WordCloud( # generate wordcloud\n",
    "    width = 400, \n",
    "    height = 400,\n",
    "    max_words = 50,\n",
    "    background_color ='white',\n",
    "    stopwords = stopset,\n",
    "    min_font_size = 10\n",
    ").generate_from_frequencies(word_feed)\n",
    "\n",
    "plt.figure(figsize = (5, 5), facecolor = None)\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.savefig('word_cloud/example.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e35655",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_perm_imp = most_perm_imp.perm_imp_tot.min() # look at what the minimal importance score is for each theme\n",
    "most_perm_imp['perm_imp_coef'] = most_perm_imp.perm_imp_tot.apply(lambda val: np.floor(np.log2(val//min_perm_imp))+1)\n",
    "anagram_feed = {row['feature'] : row['perm_imp_coef'] for i, row in most_perm_imp.iterrows()}\n",
    "anagram_cloud = WordCloud( # generate anagram cloud\n",
    "    width = 400, \n",
    "    height = 400,\n",
    "    max_words = 50,\n",
    "    background_color ='white',\n",
    "    stopwords = stopset,\n",
    "    min_font_size = 10).generate_from_frequencies(anagram_feed)\n",
    "\n",
    "plt.figure(figsize = (5, 5), facecolor = None)\n",
    "plt.imshow(anagram_cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.savefig('anagram_cloud/example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ffb60",
   "metadata": {},
   "source": [
    "Thus ends the functionality of the anagram cloud. Feel free to modify this notebook so that multiple labels may be simultaneously compared beside one another!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
